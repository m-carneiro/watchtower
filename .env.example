# Watchtower Environment Configuration
# Copy this file to .env and fill in your API keys

# AlienVault OTX API Key (optional)
# Get your key from: https://otx.alienvault.com/api
# If not set, OTX feed will be skipped
OTX_API_KEY=

# Database URL (default works with docker-compose)
# DATABASE_URL=postgres://admin:secretpassword@localhost:5432/watchtower

# Slack Configuration (optional - for notifications)
# Get your bot token from: https://api.slack.com/apps
SLACK_BOT_TOKEN=
SLACK_SIGNING_SECRET=
SLACK_CHANNEL_SECURITY=#security-alerts
SLACK_MENTION_TEAM=@security-team

# SentinelOne Configuration (optional - for webhook integration)
# Shared secret for validating SentinelOne webhooks
SENTINELONE_WEBHOOK_SECRET=

# REST API Configuration
REST_API_PORT=8080
REST_API_AUTH_TOKEN=your-api-token-here

# gRPC API Configuration (for internal services)
# SECURITY: Default is localhost:50051 for security (only local connections)
# Set to 0.0.0.0:50051 to allow external connections (use with firewall)
# GRPC_LISTEN_ADDR=localhost:50051

# Notification Settings
NOTIFY_HIGH_CONFIDENCE_THRESHOLD=80
NOTIFY_SUPPLY_CHAIN=true
NOTIFY_SENTINELONE_DETECTIONS=true

# SIEM Configuration
SIEM_FEED_ENABLED=true
SIEM_FEED_FORMAT=cef
SIEM_FEED_AUTH_TOKEN=siem-api-token-here

# LLM Triaging Configuration (optional - for AI-powered threat analysis)
# Enable intelligent threat triaging with LLM (OpenAI, Anthropic, etc.)
LLM_TRIAGE_ENABLED=false

# LLM API Configuration
# For OpenAI: https://api.openai.com/v1/chat/completions
# For LiteLLM Proxy: http://localhost:4000/chat/completions (supports multiple providers)
# For Azure OpenAI: https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT/chat/completions?api-version=2023-05-15
LLM_API_URL=https://api.openai.com/v1/chat/completions

# LLM API Key
# For OpenAI: Get from https://platform.openai.com/api-keys
# For Anthropic: Get from https://console.anthropic.com/
# For Azure: Use your Azure OpenAI key
LLM_API_KEY=

# LLM Model
# OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229
# Azure: Your deployment name
LLM_MODEL=gpt-4o-mini

# LLM Guardrails Configuration (optional - uses defaults if not set)
# Minimum confidence (0-100) required to mark an alert as false positive
# Higher values = more conservative (fewer false positives marked)
# Recommended: 85-95
LLM_GUARDRAIL_MIN_FP_CONFIDENCE=85

# Require threat intelligence match for critical severity alerts
# Set to false if you want LLM to assign critical without threat intel
# Recommended: true (prevents over-escalation)
LLM_GUARDRAIL_REQUIRE_INTEL_FOR_CRITICAL=true

# Maximum severity allowed without threat intelligence match
# Options: critical, high, medium, low, info
# Recommended: medium (prevents high/critical without evidence)
LLM_GUARDRAIL_MAX_SEVERITY_WITHOUT_INTEL=medium

# LLM Circuit Breaker Configuration (optional - resilience features)
# Enable circuit breaker to protect against cascading failures
# Set to false to disable (enabled by default for protection)
LLM_CIRCUIT_BREAKER_ENABLED=true

# Max consecutive failures before opening circuit
# Recommended: 5
LLM_CIRCUIT_BREAKER_MAX_FAILURES=5

# Timeout in seconds before attempting to close circuit
# Recommended: 30
LLM_CIRCUIT_BREAKER_TIMEOUT_SECONDS=30

# LLM Retry Configuration (optional - resilience features)
# Maximum retry attempts for transient errors (5xx, timeouts)
# Set to 0 to disable retry logic
# Recommended: 3
LLM_RETRY_MAX_ATTEMPTS=3

# Initial retry interval in milliseconds
# Recommended: 500
LLM_RETRY_INITIAL_INTERVAL_MS=500

# Maximum retry interval in milliseconds (with exponential backoff)
# Recommended: 5000
LLM_RETRY_MAX_INTERVAL_MS=5000

# Additional optional provider API keys can be added here in the future
# Example:
# VIRUSTOTAL_API_KEY=
# SHODAN_API_KEY=
